# Crunchy PostgreSQL Tile

## Plans

The Crunchy PostgreSQL Tile offers several out of the box (configurable) plans.  When deploying your Crunchy PostgreSQL Tile, operators have the opportunity to tune these plans for their enviornment needs.

### Sizes

The plans that are included are as follows:

* Small
* Medium
* Large
* XLarge
* Custom (Not available currently)

### Recommendations

The following table suggests recommendations for configuring plans.  These recommendations will not accomodate every application, however, provides a starting point for operators to configure their tile.

| Plan   | vCPU | Memory | Disk  |
|--------|------|--------|-------|
| Small  | 4    | 32GB   | 64GB  |
| Medium | 8    | 64GB   | 128GB |
| Large  | 16   | 128GB  | 256GB |
| XLarge | 32   | 256GB  | 512GB |

**NOTE**: Part of High Availability is having room to grow the database.  Operators and developers should consider the rate at which they expect to grow.

### Cluster Configurations

The following table provides information about the out-of-box cluster configurations:

| Plan   | Replica | Connections |
|--------|---------|-------------|
| Small  | 1       | 100         |
| Medium | 1       | 100         |
| Large  | 1       | 200         |
| XLarge | 1       | 500         |

**NOTE**: These are not yet configurable.

**NOTE**: There is a current limitation for only one replica at this time.

**NOTE**: All replicas use `async` replication, currently `sync` replication is not yet configurable.

## Automated Backups

The Crunchy PostgreSQL Tile uses [pgBackrest](https://github.com/pgbackrest/pgbackrest) as a dedicated backup and archiving host.  The tile comes pre-configured with nightly physical backups of the database server:

| Day       | Backup Type | Time     |
|-----------|-------------|----------|
| Sunday    | Full        | 1 am UTC |
| Monday    | Incremental | 1 am UTC |
| Tuesday   | Incremental | 1 am UTC |
| Wednesday | Incremental | 1 am UTC |
| Thursday  | Incremental | 1 am UTC |
| Friday    | Incremental | 1 am UTC |
| Saturday  | Incremental | 1 am UTC |

Although backups only happen once a day, PostgreSQL is continuously shipping the Write-Ahead-Logs (WAL) to the pgBackrest server.  This means that point-in-time recovery is possible, regardless of the schedule.

These backups not only offer a peace of mind but are used in the tile frequently.  The Crunchy PostgreSQL Tile uses backups to create replicas in the stack.  By using backups in operations, we can ensure that backups and restores work.

## Load Balancing

Crunchy PostgreSQL Tile uses `HAProxy` as the single point of entry to the database cluster.  Application developers must switch ports depending on the type of cluster they want to interact with.

### Reads vs Writes

To query a `replica`, applications must use the `5433` port on the load balancer.  This will ensure that reads are redirected to the replica cluster.

To use the `primary` cluster, application must use the `5432` port on the load balancer.  This will ensure that writes or reads are redirected to the primary cluster.

By using port switching, applications have the ability to manage the types of database interactions their application needs.  When used correctly, this strategy allows applications to be more perfomant.

## HA Model

The Crunchy PostgreSQL Tile provisions a cluster of PostgreSQL servers and self configures their roles (primary and many replicas).  This allows the servers to change their roles when failures are detected.

Throughout the Crunchy PostgreSQL Tile, configuration files are managed by `consul-template`.  Each of these templates watch different parts of the stack.  When changes are detected, configuration files automatically are rendered with the latest state.  This allows the system to be dynamic and change depending on state of services.

For example, the PostgreSQL load balancer will automatically detect when replicas are added/removed and will reconfigure its pool to reflect the current state.

### Auto Failover

Crunchy Cluster Manager runs on each of the Consul Servers within the Crunchy PostgreSQL Tile.  The job of CCM is to detect failures of the PostgreSQL servers and to determine who is the best candidate to replace a failed primary.  CCM measures replication lag to determine the best candidate to elect for the primary role.

Once a new primary role is elected, CCM updates the Consul Service Catalog to reflect the new state.  The newly elected primary will reconfigure itself (trigger a failover) and all other services will detect the new primary.

Failed former primaries are put into a fenced state.  This tells the rest of the stack to no longer communicate with the failed service.  An hourly `cron` job will attempt to repair fenced servers and add them to the replica pool.

![Model](http://i.imgur.com/sxamhbn.png)

## Tools

The Crunchy PostgreSQL Tile offers two tools to help developers manage their system:

* cf-pgadmin4
* cf-pgloader
